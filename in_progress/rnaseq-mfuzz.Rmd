---
title: Clustering RNAseq data using the mfuzz package for fuzzy c-means clustering.
author: Jake Warner
date: '2018-07-31'
slug: clustering-rnaseq-data-using-mfuzz
categories:
  - BioInformatics
tags:
  - clustering
  - R
  - RNAseq
---

Note this is part 4 of a series on clustering RNAseq data. Check out [part one  on hierarcical clustering here](http://www.2-bitbio.com/2017/04/clustering-rnaseq-data-making-heatmaps.html), or [part two on K-means clustering here](http://www.2-bitbio.com/2017/10/clustering-rnaseq-data-using-k-means.html), or [part three on fuzzy c-means clustering here](http://www.2-bitbio.com/post/clustering-rnaseq-data-using-fuzzy-c-means-clustering/).

Clustering gene expression is a fundamental step in RNAseq analyses. In this series on clustering we've talked about several different approaches to gene expression clustering including, [hierarcical clustering](http://www.2-bitbio.com/2017/04/clustering-rnaseq-data-making-heatmaps.html), [K-means clustering here](http://www.2-bitbio.com/2017/10/clustering-rnaseq-data-using-k-means.html), and [fuzzy c-means clustering here](http://www.2-bitbio.com/post/clustering-rnaseq-data-using-fuzzy-c-means-clustering/). In the previous post on fuzzy c-means clustering we built our workflow around the `cmeans` algorithm in the `e1071` package. As mentioned in that post, there is a package with several useful functions built around the same algortihm, [mfuzz](https://www.bioconductor.org/packages/release/bioc/html/Mfuzz.html). 

Mfuzz certainly is convenient and I've used it in pervious publications because it is built specifically for time-series data. I particularly like it's tuneability and helper functions for selecting optimum cluster number. I'm not a huge fan however of it's use of the Expression Set data format (rather than a simple table) and as an avid ggplotter I find it's plotting functions a bit limiting. Those gripes aside it is still an awesome package especially for those getting their feet wet in timeseries analyses.

Here we'll take some of the major functions of this package on a test drive.

## How to use the mfuzz package with RNAseq data
For this tutorial we'll use the same dataset we used previously in the post on [fuzzy c-means clustering](http://www.2-bitbio.com/post/clustering-rnaseq-data-using-fuzzy-c-means-clustering/). First we download the data, apply a quick normalization and, for the sake of this tutorial and computation time, we'll take a subset of the most highly expressed and variable genes. I'm also injecting a random sample of low variance genes to see how the clustering algorithms hold up to this noise.
```{r warning=F,message=FALSE}
library(edgeR)
library(SummarizedExperiment)
load(url("http://duffel.rail.bio/recount/SRP049355/rse_gene.Rdata"))
counts <- assays(rse_gene)$counts
y <- as.matrix((counts))
y <- DGEList(counts = y, group=c(1,2,3,4,5,6,7,8,9,10))
y <- calcNormFactors(y)
z <- cpm(y, normalized.lib.size=TRUE)

#mean/variance calculations
z_var <- apply(z, 1, var)
z_mean <- apply(z, 1, mean)

#take only the most highly express/ variable
#this is just for the sake of this tutorial
test_data <- z[which(z_var > 50 & z_mean > 50), 6:10]
```

### Should you filter the data?
As I touched on briefly in the [previous post](http://www.2-bitbio.com/post/clustering-rnaseq-data-using-fuzzy-c-means-clustering/), since fuzzy c-means is noise robust we could perform an *a posteriori* filter using the membership scores. This has the benefit of not exluding genes which have an interesting profile but don't make a perviously designated filtering threshold.

## Importing the data:
The first thing we need to do is import the data as an expression set object. Another pecularity of the mfuzz package is it needs a time vector sitting in the top row of the data frame. We'll add that, then convert it to an expression set using the `table2eset` function.
```{r warning=F,message=FALSE}
library(Mfuzz)
library(reshape)
test_data <- rbind(c(1,2,3,4,5), test_data)
row.names(test_data)[1]<-"time"

write.table(test_data, file = "test_data.txt", sep='\t', quote = F, col.names=NA)
#drop the uncut case
#fixed the time header for Mfuzz in text edit
data_eset <- table2eset(file="test_data.txt")
```

## Scaling the data:
This is where we'd normally scale the data and center it. Mfuzz provides a nice function for this:
```{r warning=F,message=FALSE}
data_eset_s <- standardise(data_eset)
```

## Fuzzy c-means: Estimate the fuzzifier
Fuzzy c-means relies on a fuzzifier parameter to designate how 'fuzzy' the clusters are. [Schwaemmle and Jensen described a method for estimating the fuzzifier](https://www.ncbi.nlm.nih.gov/pubmed/20880957) and this is wrapped into a function for this package:
```{r warning=F,message=FALSE}
m1 <- mestimate(data_eset_s)
m1
```

## Fuzzy c-means: How many clusters?
Fuzzy c-means clustering requires *a priori* knowledge of the number of clusters. The number of clusters can really impact the classifications so it's an important consideration. We discussed this problem and provide various solutions in [the post on k-means clustering](http://www.2-bitbio.com/2017/10/clustering-rnaseq-data-using-k-means.html). Too many clusters and the algorithm will have a difficult time classifying genes into their appropriate cluster and the cluster shapes will be redundant. Too few clusters and distinct cluster shapes will be collapsed inappropriately.

The mfuzz package provides a few functions to help with this including `Dmin`. This function performs repeated soft clustering with increasing k number of clusters and calculates the distance of the centroids for each cluster. In theory, there will be an inflection point at which the minimum centroid distances does not decrease significantly with additional clusters. In other words, with more clusters we're just subdividing well spearated clusters into smaller pieces.
```{r warning=F,message=FALSE,fig.height= 5, fig.width = 5}
Dmin(data_eset_s, m=m1, crange=seq(2,22,1), repeats=3, visu=TRUE)
```

In the [previous post](http://www.2-bitbio.com/post/clustering-rnaseq-data-using-fuzzy-c-means-clustering/) I used the within sum of squared error as a metric for selecting cluster number which I prefer slightly to using minimum centroid distance. But they really aren't that different.

Here's looking at the WSS, note that this function is not included in the package:
```{r warning=F,message=FALSE}
library(e1071)
#helper function for the within sum of squared error
sumsqr <- function(x, clusters){
  sumsqr <- function(x) sum(scale(x, scale = FALSE)^2)
  wss <- sapply(split(as.data.frame(x), clusters), sumsqr)
  return(wss)
}

#get the wss for repeated clustering
iterate_fcm_WSS <- function(df,m){
  totss <- numeric()
  for (i in 2:20){
    FCMresults <- cmeans(df,centers=i,m=m)
    totss[i] <- sum(sumsqr(df,FCMresults$cluster))
  }
  return(totss)
}

#
data_eset_df <- data.frame(exprs(data_eset_s))
wss_2to20 <- iterate_fcm_WSS(data_eset_df,m1)
plot(1:20, wss_2to20[1:20], type="b", xlab="Number of Clusters", ylab="wss")
```

First we'll look the centroid profiles:
```{r warning=F,message=FALSE,fig.width = 6}
#import some data manipulation functions
library(reshape2)
library(tidyr)
library(dplyr)

#get the centroids into a long dataframe:
fcm_centroids <- fcm_results$centers
fcm_centroids_df <- data.frame(fcm_centroids)
fcm_centroids_df$cluster <- row.names(fcm_centroids_df)
centroids_long <- tidyr::gather(fcm_centroids_df,"sample",'value',1:5)

ggplot(centroids_long, aes(x=sample,y=value, group=cluster, colour=as.factor(cluster))) + 
  geom_line() +
  xlab("Time") +
  ylab("Expression") +
  labs(title= "Cluster Expression by Time",color = "Cluster")
```

So we have some interesting patterns! The cluster centroids are well separated although there might be some redundancy as a couple of the profiles look very similar.

We can assess the similarity of the cluster centroids to identify redundancy or high overlap. To do we simply correlate the cluster centroids with each other. If the centroids are too similar then they will have a high correlation. If your K number produces clusters with high correlation (say above 0.85) then consider reducing the number of clusters.

Correlate the centroids to see how similar they are:
```{r warning=F,message=FALSE}
cor(t(fcm_centroids))
```

It looks like we have good separation of the clusters as no cor score is above 0.85.

Now's the fun part where we can plot the gene profiles by cluster. We simply subset the results and the centroids by cluster and plot it out. We can also set a color scale for the membership score:
```{r warning=F,message=FALSE,fig.width = 6}
#start with the input data
fcm_plotting_df <- data.frame(scaledata)

#add genes
fcm_plotting_df$gene <- row.names(fcm_plotting_df)

#bind cluster assinment
fcm_plotting_df$cluster <- fcm_results$cluster
#fetch the membership for each gene/top scoring cluster
fcm_plotting_df$membership <- sapply(1:length(fcm_plotting_df$cluster),function(row){
  clust <- fcm_plotting_df$cluster[row]
  fcm_results$membership[row,clust]
})

k_to_plot = 1

#subset the dataframe by the cluster and get it into long form
#using a little tidyr action
cluster_plot_df <- dplyr::filter(fcm_plotting_df, cluster == k_to_plot) %>%
  dplyr::select(.,1:5,membership,gene) %>%
  tidyr::gather(.,"sample",'value',1:5)

#order the dataframe by score
cluster_plot_df <- cluster_plot_df[order(cluster_plot_df$membership),]
#set the order by setting the factors using forcats
cluster_plot_df$gene = forcats::fct_inorder(cluster_plot_df$gene)

#subset the cores by cluster
core <- dplyr::filter(centroids_long, cluster == k_to_plot)

ggplot(cluster_plot_df, aes(x=sample,y=value)) + 
    geom_line(aes(colour=membership, group=gene)) +
    scale_colour_gradientn(colours=c('blue1','red2')) +
    #this adds the core 
    geom_line(data=core, aes(sample,value, group=cluster), color="black",inherit.aes=FALSE) +
    xlab("Time") +
    ylab("Expression") +
    labs(title= paste0("Cluster ",k_to_plot," Expression by Time"),color = "Score")

```

In this plot, genes with a profile close to the core have a membership score approaching 1 (red) while those with divergent patterns have a score closer to 0 (blue). You can see there is some noise but the genes mostly fit the cluster model. If you observe many genes with low scores consider increasing your K as they've been 'forced' into a cluster in which they don't belong. Too much noise in the data can also lead to low scoring genes.

### Should you filter the data (redux)?
This is the point where I'd filter the data based on membership score before proceeding to downstream analysis. This will leave out genes that don't fit well into any cluster, which are probably noise, before proceeding.

##Comparing cluster methods: 
Let's see how this compares to hard K-means clustering!

First we perform the kmeans:
```{r warning=F,message=FALSE}
#perform the kmeans
kmeans_results <- kmeans(scaledata,centers=5)
```

Now we will use the WGCNA function `matchLabels` to line up the cluster assignments. This doesn't change the structure of the cluster, but attempts to rename the cluster based on the cluster assignment from another dataset. Then we calculate the cluster overlap using the `overlapTable` function.

```{r message=F,fig.width = 6}
#these functions from the WCGNA package are great for this:
source('https://raw.githubusercontent.com/cran/WGCNA/master/R/matchLabels.R')
source('https://raw.githubusercontent.com/cran/WGCNA/master/R/accuracyMeasures.R')

#grab the cluster vector for kmeans
kmeans_clusters <- kmeans_results$cluster
#grab the cluster vector for fuzzy
fcm_clusters <- fcm_results$cluster

#grab the cluster vector
kmeans_clusters_matched <- matchLabels(kmeans_clusters,fcm_clusters)

#add a prefix so we can tell them apart:
kmeans_clusters_matched <- paste0('K-',kmeans_clusters_matched)
fcm_clusters <- paste0('FCM-',fcm_clusters)

#calculate the overlap
OT<- overlapTable(kmeans_clusters_matched, fcm_clusters)
#get rid of 0 values...
OT$pTable[OT$pTable == 0] <- 2e-300

textMatrix= paste(signif(OT$countTable, 2), "\n(",
                      signif(OT$pTable, 1), ")", sep= "")
dim(textMatrix)= dim(OT$countTable)
#par(mar=c(10,10,10,10))
library(gplots)
heatmap.2(x= -log(OT$pTable),
          dendrogram = "none",
          Colv =F,
          Rowv = F,
          scale = c("none"),
          col="heat.colors",
          na.rm=TRUE,
          cellnote = textMatrix,
          notecol="grey30",
          notecex=0.6,
          trace=c("none"),
          cexRow = 0.8,
          cexCol = 0.8,
          main = "Cluster-Cluster Overlap",
          xlab = "Fuzzy C-means (k=5)",
          ylab = "Kmeans (k=5)")
```

As you can see there is some discrepancy in how the two algorithms behave but overall they perform similarly. Since we enriched our dataset in the most highly variable genes we would expect the clustering to be fairly robust in both cases. As we noted above, fuzzy c-means performs better in noisier datasets which might be more typical than our practice data here.

## What to do from here?
Once we are happy with our clustering we can do lots of analyses on the clustered dataset. Including but not limited to:

* Correlate phenotypic data with our clusters (can use the centroids for this).
* Perform gene set enrichmnet analysis on our clusters (GSEA).
* Test for gene ontology (GO) term enrichment in our clusters.
* Compare cluster membership between datasets.
* Analyze the highest scoring genes within clusters (core genes).

Hope this helps! Tune in for part four of this series on clustering when we take the mFuzz package for a test drive!

```{r session}
sessionInfo()
```
